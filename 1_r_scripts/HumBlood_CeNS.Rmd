---
title: "Comparative Hummingbird Blood: Cell Size vs. Cell Number (CeNS) Analyses"
author: "Jessie Williamson"
date: "4/26/2021; last revised 4/30/21"
output: html_document
---

Script to try to get to the bottom of 'cell size vs. cell number' and create the CeNS index. 


**This script has 2 parts:** 

**1) CeNS Models:** 
    a) indiviual-level dataset
    b) species-mean dataset 
    c) single-species models for well-sampled species 
    
**2) CeNS index:** 
    - Take results of single-species models to calculate this. Formula is: TRBC estimate/(MCV estimate + TRBC estimate).
    - Is there phylo signal to these estimates? Prune trees, then work up contmaps, Pagel's lamda, and blomberg's K
    - Take these estimates and run another model: maybe try cens.index ~ mass + elev + (1|phylo) to see what predicts this index? —> This is making that censdat cool jump that Chris mentioned, where we’ll show that something additional PREDICTS our index.

ADD OTHER NOTES HERE 




DO NOT FORGET TO CONVERT STANDARDIZED ESTIMATES TO REAL-WORLD ESTIMATES 



#######


```{R, echo=FALSE}
# GLOBAL R chunk options here (hide message w/ echo=FALSE)
knitr::opts_chunk$set(comment = NA, message = FALSE, warning = FALSE, width = 100)
knitr::opts_chunk$set(fig.align = "center", fig.height = 4, fig.width = 6)
knitr::opts_chunk$set(cache = TRUE, autodep=TRUE)
```


# Load packages
```{R}
library(reshape)
library(reshape2)
library(plyr)
library(dplyr)
library(car)
library(GGally)
library(Hmisc)
library(gridExtra)
library(stats)
library(gplots)
library(ggplot2)
library(stats4) # Forces knitr to work when it's being wonky
library(PMCMR) #Allows Kruskal-Wallis post-hocs
library(effects)
library(gridExtra)
library(lattice)
# library(survival)
# library(fmsb)
library(faraway)
library(tidyverse)
library(patchwork) 
library(viridis)
library(rcompanion) # for Tukey's Ladder of Powers Transformation
library(purrr)
library(tidyr)
library(cowplot)
library(gganimate)
library(job) # Run slow brms models as jobs to free up console

# Bayesian modeling packages 
library(MCMCglmm)
library(bayesplot)
library(rstan)
library(Rcpp) # required for brms
library(brms) # updated to 2.15.0 on 3/29/21
library(bayesplot) # appears to be required for bayesplot::pp_check?
library(ggdist)
library(tidybayes)
library(performance)
library(bayestestR) # for effective sample size tests

# To run each time you load rstan
options(mc.cores = parallel::detectCores()) # for core setup 
rstan_options(auto_write = TRUE) # auto save  bare verion of compiled Stan program to HD
# rstan output should always end in a blank line w/ no characters or spaces  

# Phylo packages 
library(phytools)
# library(ape)

# Frequentist modeling packages
# library(nlme)
# library(lme4)
# library(AICcmodavg)
# library(MuMIn)
# library(glmulti)
# library(lsmeans)
# library(rsq) # get r-squared values from GLM
# library(r2glmm) # for R^2 values from lmer() and glmer()
# library(multcompView) # related to multiple comparisons?
# library(jtools) # interaction plots 
# library(interactions) # interaction plots 
# library(broom)
# library(stargazer) # model output tables
# library(ggeffects) # for estimating model predictions from mixed effects models
```


---

# Clear workspace and set WD
```{R}
rm(list=ls(all=TRUE)) # clear workspace 
setwd("/Users/Jessie/Dropbox (MSBbirds)/Rdirectory/ComparativeHummingbirdBlood")
```


# Load in datasets
```{R}
# Load functions 
source("/Users/Jessie/Dropbox (MSBbirds)/Rdirectory/ComparativeHummingbirdBlood/1_r_scripts/ada_functions.R") 
  # Erik's ADA functions for clean & collated lm diag plots
source("/Users/Jessie/Dropbox (MSBbirds)/Rdirectory/ComparativeHummingbirdBlood/1_r_scripts/Rfunctions.R") # Nora's functions

# Final hummingbird blood dataset (remember that you completed pre-processing in HumBlood_Modeling file)
final <- read.csv("ComparativeHummingbirdBlood_FinalDataset_ForModeling_04-26-21.csv", stringsAsFactors = FALSE)
  # Keep stringsasfactors=FALSE because species and dept need to be characters in order for Patagona rename loop to work
final <- final[ , !(colnames(final) %in% c("X"))] # drop weird X column 1 if reading in from read.csv

# # Phylogeny pruned from McGuire and adjusted (see script HumBlood_Phylogeny.Rmd)
# tree <- read.tree("McGuirePruned_AllHummingbirdComparativeTree_FINAL.tre") # a list 
# tree$tip.label # Check 77 tips
# 
# # Write out final tree file as pdf for figure-making
# # In order to write out .pdf tree file: 1) Initiate pdf (or jpeg, etc) file; 2) Plot tree w/ all graphics; 3) dev.off to save
# pdf("McGuirePruned_AllHummingbirdComparativeTree_FINAL.pdf", width=5, height=6) 
# plotTree(tree, ftype="i") # Check this pre-final tree w/ adjusted branch lengths 
# dev.off()
# 
# # Read in sub trees 
# tree.hb <- read.tree("McGuirePruned_AllHummingbirdComparative_Tree.hb_FINAL.tre")  
# tree.hct <- read.tree("McGuirePruned_AllHummingbirdComparativeTree.hct_FINAL.tre")  
# tree.trbc <- read.tree("McGuirePruned_AllHummingbirdComparativeTree.trbc_FINAL.tre") 
# tree.mcv <- read.tree("McGuirePruned_AllHummingbirdComparativeTree.mcv_FINAL.tre") 
# tree.mch <- read.tree("McGuirePruned_AllHummingbirdComparativeTree.mch_FINAL.tre") 
# tree.mchc <- read.tree("McGuirePruned_AllHummingbirdComparativeTree.mchc_FINAL.tre")
```


# Pre-processing for modeling - individual-level data 
```{r}
# Subset the data to just blood variables of interest
# We could alternatively use 'final' dataset and model will remove NAs for us, but this is a little bit cleaner. 
censdat <- subset(final, select=c(species, rowID, nk, department, elev, hb, mcv, mcv.log, trbc)) 
  # dept needed for filtering of patagona for species mean data
censdat <- na.omit(censdat) # 688 observations 

# We don't need to check normality and transform because we did that during pre-processing in HumBlood_Modeling. 
# We know MCV has heavy tails, respons well to skew_normal, but *might* fit better with mcv.log variable - we'll see below.

# Standardize predictors 
censdat$mcv.z <- standardize(censdat$mcv)
censdat$trbc.z <- standardize(censdat$trbc)

# Correlation matrix 
cor(censdat[,c("hb", "mcv.z", "trbc.z")])
# Strong negative correlation  between TRBC and MCV, but these two are very different measures and essential to 
# understanding how birds increase [Hb]; for this reason, we justify leaving both in models. 

# Loop through dataset and assign "Patagona_gigas_S" to Chilean birds and "Patagona_gigas_N" to Peru birds
# Species and Department MUST be characters in order for this to work 
for(i in 1:nrow(censdat)){
    if((censdat$species[i] == "Patagona_gigas") & (censdat$department[i] == "Región Valparaíso") ){
        censdat$species[i] <- "Patagona_gigas_S"
    }else if((censdat$species[i] == "Patagona_gigas")){ 
      censdat$species[i] <- "Patagona_gigas_N"
    }
}

# # NOW check structures and convert characters to factors 
str(censdat)
censdat$species <- as.factor(censdat$species)
censdat$department <- as.factor(censdat$department)

# Make department names consistent; remove 'í' characters in case R is reading them in incorrectly
# And must do this here after as.factor() because this only works on FACTORS 
levels(censdat$department)[levels(censdat$department)=="Valparaíso"] <- "Valparaiso"
levels(censdat$department)[levels(censdat$department)=="Región Valparaíso"] <- "Valparaiso"

# Predictor dataset - take a look at distributions
# p <- ggpairs(subset(censdat, select = c(hb, mcv, trbc, mcv.log, mcv.z, trbc.z))); print(p)
```


# Pre-processing for modeling - species mean data 
```{r}
cens.sp <- censdat # Make the final dataset copy that you'll use to wrangle species mean data 
str(cens.sp)

# Drop species with fewer than 3 records 
cens.sp <- subset(cens.sp, with(cens.sp, species %in% names(which(table(species)>=3)))) 

# species mean summary - should be no NAs here 
cens.sp <- cens.sp %>% group_by(species) %>% summarise(hb = mean(hb),   
                                                  elev = mean(elev),   
                                                  mcv = mean(mcv),
                                                  mcv.log = mean(mcv.log),
                                                  trbc = mean(trbc)
                                                  ) 

# Standardize predictors 
cens.sp$mcv.z <- standardize(cens.sp$mcv)
cens.sp$trbc.z <- standardize(cens.sp$trbc)

# This dataset now contains species mean values for Hb, TRBC, MCV for all species with 3 or more individuals sampled. 
```


# Pre-processing for single-species model sets 
```{r}
# Need to ID min number of indiv/species necessary to run models- 6? 10? 
# Subset from censdat, wich has NAs removed 
# Will need to run a standardized model *and* unstandardized model per species miniset (I think)
# BUT, if goal is "proportional contribution of TRBC relative to MCV in driving [Hb]", I think we want unstandardized...
# Remember that you have 2 functional groups for Patagona; will need to drop gigas_S if working up phylo signal
# Quick test w/ Sephanoides sephaniodes at 5 looks like too few 
# As a general rule, should be at least 10 observations per variable, so we'll use n=10 as our cutoff 
# Might consider doing as EBL did and making a big loop to run through the 30 species-specific models 

# Get dataset counts 
sp.count.cens <- censdat %>% group_by(species) %>% summarise(count = length(nk)) # Observations/species for censdat

# Make a species-specific modeling dataset
cens.ss <- subset(censdat, with(censdat, species %in% names(which(table(species)>=10))))
ss.count.cens <- cens.ss %>% group_by(species) %>% summarise(count = length(nk)) # Obs/species for ss dataset

# Split data frame by species and dump into a massive list
cens.ss.list <- cens.ss %>% group_split(species)

# Now write individual data frames for input into models 
Amel <- as.data.frame(test[[1]]); str(Amel) # check structure on this first data frame to verify correct
Acas <- as.data.frame(test[[2]])
Akin <- as.data.frame(test[[3]])
Aama <- as.data.frame(test[[4]])
Clar <- as.data.frame(test[[5]])
Coen <- as.data.frame(test[[6]])
Ccoe <- as.data.frame(test[[7]])
Ciri <- as.data.frame(test[[8]])
Cvio <- as.data.frame(test[[9]])
Ccor <- as.data.frame(test[[10]])
Eluc <- as.data.frame(test[[11]])
Econ <- as.data.frame(test[[12]])
Fmel <- as.data.frame(test[[13]])
Ghir <- as.data.frame(test[[14]])
Hame <- as.data.frame(test[[15]])
Hmic <- as.data.frame(test[[16]])
Hlea <- as.data.frame(test[[17]])
Llaf <- as.data.frame(test[[18]])
Lnun <- as.data.frame(test[[19]])
Meup <- as.data.frame(test[[20]])
Mpho <- as.data.frame(test[[21]])
Mtyr <- as.data.frame(test[[22]])
Ound <- as.data.frame(test[[23]])
Oest <- as.data.frame(test[[24]])
Omel <- as.data.frame(test[[25]])
Pgign <- as.data.frame(test[[26]])
Pgigs <- as.data.frame(test[[27]])
Pguy <- as.data.frame(test[[28]])
Pmal <- as.data.frame(test[[29]])
Tfur <- as.data.frame(test[[30]])

# Pguy <- censdat[which(censdat$species == "Phaethornis_guy"),] # another clunky subset alternative

# Predictor dataset - take a look at distributions
# p <- ggpairs(subset(Pguy, select = c(hb, mcv, trbc, mcv.log, mcv.z, trbc.z))); print(p)
# p <- ggpairs(subset(Mtyr, select = c(hb, mcv, trbc, mcv.log, mcv.z, trbc.z))); print(p)
```


Set global plot theme 
```{r}
# Set global plot theme
theme_set(theme_bw() +
            theme(
              plot.background = element_blank()
              ,panel.grid.major = element_blank()
              ,panel.grid.minor = element_blank()
              ,panel.background = element_blank()
              ,axis.text.x  = element_text(angle=90, vjust=0.5, size=8)
            ))
```


#### CeNS MODELING #####

Three broad model categories 
1) indiviual-level (il) dataset (all data from all individuals of all species; no elimination of sampling outliers)
2) species-mean (sm) dataset (drop species with < 3 individuals sampled, calculate species means, run models w/ this dataset)
3) single-species (ss) models for well-sampled species (figure out threshold min value of individuals needed to run these)
  --> Result from single-species models will form CeNS index


### 1) Individual-level CeNS models - What explains CeNS variation within species? 

# Set up and run models
We'll fit two models for our individual-level data: 
1) Intercept only null model
2) "Full" model hb ~ MCV + TRBC + (1|species)
The species grouping variable is essential to our model design because it will allow us to account for repeated sampling within species (i.e., will hold species identity constant) while estimating variation within species.

Note: on 5/13/21 I removed the intercept only + (1|species) model for ease of comparison in the two model format with the species mean dataset. 
```{r}
# INTERCEPT ONLY 
job::job({ # Send this to a job so you can use the console while it runs
cens.il.m1 <- brm(
  formula = bf(hb ~ 1),
  data = censdat,
  family = gaussian(), 
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
  iter = 10000, 
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  sample_prior = TRUE # save priors
)
save(cens.il.m1, file="cens.il.m1_InterceptOnly.RData") # save model
# load("cens.il.m1_InterceptOnly.RData") # If loading from pre-saved file and not re-running

# # INTERCEPT + SPECIES GROUPING VARIABLE 
# cens.il.m2 <- brm(
#   formula = bf(hb ~ 1 + (1|species)),
#   data = censdat,
#   family = gaussian(),
#   cores = 4,
#   chains = 4,
#   thin = 10,
#   warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
#   iter = 10000, 
#   control = list(adapt_delta = 0.99, max_treedepth = 15),
#   sample_prior = TRUE # save priors
# )
# save(cens.il.m2, file="cens.il.m2_SpeciesREOnly.RData") # save model
# # load("cens.il.m2_SpeciesREOnly.RData") 

# PREDICTORS AND SPECIES GROUPING VARIABLE 
cens.il.m2 <- brm(
  formula = bf(hb ~ 1 + mcv.z + trbc.z + (1|species)),
  data = censdat,
  family = gaussian(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
  iter = 10000,
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  sample_prior = TRUE # save priors
)
save(cens.il.m2, file="cens.il.m2_Predictors_SpeciesRE.RData") # save model
# load("cens.il.m2_Predictors_SpeciesRE.RData")
})
```


# Cens individual-level: Summarize and check fit 
Explanation of what mcmc_plot output below means: https://cran.r-project.org/web/packages/bayesplot/vignettes/plotting-mcmc-draws.html
```{r}
# FIT AND CONVERGENCE 
mcmc_plot(cens.il.m2, type = "trace") 
plot(cens.il.m2, N = 2, ask = FALSE)
bayesplot::pp_check(cens.il.m2, resp = "hb", nsamples = 100) # pull 100 draws from model and cast back to original data 
bayesplot::pp_check(cens.il.m2, type = "stat", stat = 'median', nsamples = 100)
bayesplot::pp_check(cens.il.m2, nsamples = 200, type = "stat_2d") + theme_bw(base_size = 20) # nice estimates
mcmc_plot(cens.il.m2, type = "acf_bar") # looks at autocorrelation
plot(conditional_effects(cens.il.m2), points = TRUE) # conditional effects 

# Quick plot of estimates and 95% CIs (default is 95% CI; adjust prob to get other CIs)
mcmc_plot(cens.il.m2)  

# Residual plots 
# typical residual plot but w/ addition of uncertainty associated w/ each residual given by generating draws from the dist 
# associated with each residual
censdat %>%
  add_residual_draws(cens.il.m2) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

censdat %>%
  add_predicted_draws(cens.il.m2) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()
# If predicted distribution is uniform, probabilities should be well calibrated
```


# Blood index: Plot outputs of posterior mean estimates and 95% credible intervals (full models, m1-m6)
```{r}
# Plot these nicely and without intercepts, which throws off scale: 
# Circle = point estimate (model summary, print(hb.m6))
# Thin bars = default 90% CI; adjust prob_outer=0.95 to get 95% CI 
# Thick bars = default 50% CI; adjust prob=0.50 to get anything else 
# x-axis should be scale based on value of estimates coming out of the model 

# Model 3 
#pairs(hb.m3)
color_scheme_set("blue") # nice default
cens.il.m2.p1 <- mcmc_plot(cens.il.m2, pars=c("b_mcv.z", "b_trbc.z"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.il.m2: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.il.m2.p1
ggsave(cens.il.m2.p1, filename = "cens.il.m2.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

summary(cens.il.m2)
# Both MCV and TRBC have strong positive effects on Hb.
# As TRBC increases, Hb increases. As MCV increases, Hb increases.
# Effect of TRBC is marginally stronger than MCV (estimates: TRBC = 2.04; MCV = 1.94)
# So, it seems that individuals increase Hb by increasing TRBC. 
```


# Cens individual-level: Collate model summaries and WAIC scores 
```{r}
# Print model summaries 
sink("CeNS_IndividualLevel_brms_model_summaries.txt",append = TRUE)
summary(cens.il.m1, waic=TRUE)
summary(cens.il.m2, waic=TRUE)
sink()
# print(hb.test, prob=0.95) # change CI cut-off by adjusting prob; remember Bayesian default may not be 95%

# Get model WAIC scores  
bi.m1.waic <- waic(cens.il.m1)
bi.m3.waic <- waic(cens.il.m2)
cens.il.waics <- cbind(bi.m1.waic, bi.m3.waic); cens.il.waics
write.csv(cens.il.waics, "CeNS_IndividualLevel_brms_models_waics.csv") # totally unintelligible cumbersome doc w/ all pointwise comparisons 
```


# Cens - Individual level: Calculate Intraclass correlation coefficient (ICC) explained by species grouping variable 
Calculated from Burkner's phylo  brms tutorial: https://cran.r-project.org/web/packages/brms/vignettes/brms_phylogenetics.html
Note from Burkner: Note that the phylogenetic signal is just a synonym of the intra-class correlation (ICC) used in the context phylogenetic analysis.
```{R}
# CALCULATE INTRACLASS CORRELATION (ICC) EXPLAINED BY SPECIES GROUPING VARIABLE 
# ICC = Tells you proportion of total variance in response variable that is acccounted for by species identity (i.e. clustering)

# ICC for full model 
hyp.cens.il.m2 <- "sd_species__Intercept^2 / (sd_species__Intercept^2 + sigma^2) = 0"
(hyp.cens.il.m2 <- hypothesis(cens.il.m2, hyp.cens.il.m2, class = NULL))
plot(hyp.cens.il.m2)
# 13% variance explained; est = 0.13, CI = 0.06-0.21

#performance::variance_decomposition(cens.il.m2)
# Visually check how variance changes by adding in species term and examining plots 
# PPD1 <- posterior_predict(cens.il.m2, re.form =  ~ species)
# plot(PPD1)
```


# Cens individual-level: MODEL COMPARISON 
```{r}
# leave one out cross-validation, lowest is "best"
# if 2*standard error > delta LOOIC, models aren't necessarily distinguishable

library(loo)
sink("CeNS_IndividualLevel_brms_models_all_looic.txt", append=FALSE)
LOO(cens.il.m1, cens.il.m2, reloo=FALSE) 
sink()

# Calculate Bayesian R^2:
bayes_R2(cens.il.m1) 
#bayes_R2(cens.il.m2) # 0.17
bayes_R2(cens.il.m2) # 0.54

# Model comparisons:
#            elpd_diff se_diff
# cens.il.m2    0.0       0.0 
# cens.il.m1 -251.9      19.6 

# Now re-run top model with unstandardized predictors to estimate strength of relationship between Xs and y:
job::job({ # Send to a job to free up console
cens.il.m2.unstandardized <- 
  update(cens.il.m2,
        newdata = censdat,
        formula = hb ~ 1 + mcv + trbc + (1|species),
        chains = 4, cores = 4)
save(cens.il.m2.unstandardized, file="cens.il.m2.unstandardized_Predictors_SpeciesRE.RData") # save model
})
# load("cens.il.m2.unstandardized_Predictors_SpeciesRE.RData")

summary(cens.il.m2.unstandardized)
# mcv = 0.12; trbc = 1.79; what this tells us: 
# For every unit increase in TRBC, [Hb] increases by 1.79 g/dl
# For every unit increase in MCV, [Hb] increases by 0.12 g/dl
```

Quick summary of top model comparison: 
- Model 2, full model + species grouping variable fits best by var! Variance explained by species: 13%. R^2 = 0.54. 

Standardized Estimates: 
MCV = 1.94
TRBC = 2.04

Unstandardized Estimates: 
MCV = 0.12
TRBC = 1.79
- For every increase in MCV of one fl, we expect [Hb] will increase by 0.12 g/dl. 
- For every increase in TRBC of one unit, we expect [Hb] will increase by 1.79 g/dl. 
--> This suggests that TRBC is the primary way through which birds increase [Hb], not MCV. Perhaps this is explained by increasing Hb through erythropoesis, increased cells, consistent w/ acclimatization response? 
Also might suggest that TRBC increases happen faster than MCV increeases?? Not sure we learn about timing here. 



### 2) Species mean CeNS models - What explains CeNS variation over 22 million years of hummingbird evolution?

# Set up and run models
We'll fit two models for our species mean data: 
1) Intercept only null model
2) "Full" model hb ~ MCV + TRBC 
Note: don't model w/ mcv.log. While this does make distribution seemingly perfectly gaussian, this will throw off estimates tremendously because mcv.log is obviously on a log scale. Models are robust to the slightly-less-than-gaussian distributions of raw data, so leave these as is. 
```{r}
# INTERCEPT ONLY 
job::job({ # Send this to a job so you can use the console while it runs
cens.sm.m1 <- brm(
  formula = bf(hb ~ 1),
  data = cens.sp,
  family = gaussian(), 
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
  iter = 10000, 
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  sample_prior = TRUE # save priors
)
save(cens.sm.m1, file="cens.sm.m1_InterceptOnly.RData") # save model
# load("cens.sm.m1_InterceptOnly.RData") # If loading from pre-saved file and not re-running

# PREDICTORS 
cens.sm.m2 <- brm(
  formula = bf(hb ~ 1 + mcv.z + trbc.z),
  data = cens.sp,
  family = gaussian(), # gaussian fits better than student 
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
  iter = 10000,
  control = list(adapt_delta = 0.99, max_treedepth = 15),
  sample_prior = TRUE # save priors
)
save(cens.sm.m2, file="cens.sm.m2_Predictors_SpeciesRE.RData") # save model
# load("cens.sm.m2_Predictors_SpeciesRE.RData")
})
```


# Cens species mean: Summarize and check fit 
Explanation of what mcmc_plot output below means: https://cran.r-project.org/web/packages/bayesplot/vignettes/plotting-mcmc-draws.html
```{r}
# FIT AND CONVERGENCE 
mcmc_plot(cens.sm.m2, type = "trace") 
plot(cens.sm.m2, N = 2, ask = FALSE)
bayesplot::pp_check(cens.sm.m2, resp = "hb", nsamples = 200) # pull 100 draws from model and cast back to original data 
bayesplot::pp_check(cens.sm.m2, type = "stat", stat = 'median', nsamples = 100)
bayesplot::pp_check(cens.sm.m2, nsamples = 200, type = "stat_2d") + theme_bw(base_size = 20) # nice estimates
mcmc_plot(cens.sm.m2, type = "acf_bar") # looks at autocorrelation
plot(conditional_effects(cens.sm.m2), points = TRUE) # conditional effects 

# Quick plot of estimates and 95% CIs (default is 95% CI; adjust prob to get other CIs)
mcmc_plot(cens.sm.m2)  

# Residual plots 
# typical residual plot but w/ addition of uncertainty associated w/ each residual given by generating draws from the dist 
# associated with each residual
cens.sp %>%
  add_residual_draws(cens.sm.m2) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

cens.sp %>%
  add_predicted_draws(cens.sm.m2) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()
# If predicted distribution is uniform, probabilities should be well calibrated
```


# Blood index: Plot outputs of posterior mean estimates and 95% credible intervals (full models, m1-m6)
```{r}
# Plot these nicely and without intercepts, which throws off scale: 
# Circle = point estimate (model summary, print(hb.m6))
# Thin bars = default 90% CI; adjust prob_outer=0.95 to get 95% CI 
# Thick bars = default 50% CI; adjust prob=0.50 to get anything else 
# x-axis should be scale based on value of estimates coming out of the model 

# Model 3 
#pairs(hb.m3)
color_scheme_set("blue") # nice default
cens.sm.m2.p1 <- mcmc_plot(cens.sm.m2, pars=c("b_mcv.z", "b_trbc.z"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.sm.m2: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.sm.m2.p1
ggsave(cens.sm.m2.p1, filename = "cens.sm.m2.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

summary(cens.sm.m2)
# Both MCV and TRBC have strong positive effects on Hb.
# As TRBC increases, Hb increases. As MCV increases, Hb increases.
# Effect of TRBC is marginally stronger than MCV (estimates: TRBC = 1.42; MCV = 1.37)
# So, it seems that species increase Hb by increasing TRBC. 
```


# Cens species mean: Collate model summaries and WAIC scores 
```{r}
# Print model summaries 
sink("CeNS_SpeciesMean_brms_model_summaries.txt",append = TRUE)
summary(cens.sm.m1, waic=TRUE)
summary(cens.sm.m2, waic=TRUE)
sink()
# print(hb.test, prob=0.95) # change CI cut-off by adjusting prob; remember Bayesian default may not be 95%

# Get model WAIC scores  
bi.m1.waic <- waic(cens.sm.m1)
bi.m2.waic <- waic(cens.sm.m2)
cens.sm.waics <- cbind(bi.m1.waic, bi.m2.waic); cens.sm.waics
write.csv(cens.sm.waics, "CeNS_SpeciesMean_brms_models_waics.csv") # totally unintelligible cumbersome doc w/ all pointwise comparisons 
```


# Cens species mean: MODEL COMPARISON 
```{r}
# leave one out cross-validation, lowest is "best"
# if 2*standard error > delta LOOIC, models aren't necessarily distinguishable

library(loo)
sink("CeNS_SpeciesMean_brms_models_all_looic.txt", append=FALSE)
LOO(cens.sm.m1, cens.sm.m2, reloo=FALSE) 
sink()

# Calculate Bayesian R^2:
bayes_R2(cens.sm.m1) 
bayes_R2(cens.sm.m2) # 0.59

# Model comparisons:
#            elpd_diff se_diff
# cens.sm.m2   0.0       0.0  
# cens.sm.m1 -21.8       5.1    

# Now re-run top model with unstandardized predictors to estimate strength of relationship between Xs and y:
job::job({ # Send to a job to free up console
cens.sm.m2.unstandardized <- 
  update(cens.sm.m2,
        newdata = cens.sp,
        formula = hb ~ 1 + mcv + trbc,
        chains = 4, cores = 4)
save(cens.sm.m2.unstandardized, file="cens.sm.m2.unstandardized_Predictors_SpeciesRE.RData") # save model
})
# load("cens.sm.m2.unstandardized_Predictors_SpeciesRE.RData")

summary(cens.sm.m2.unstandardized)
# mcv = 0.14; trbc = 2.13; what this tells us: 
# For every unit increase in TRBC, [Hb] increases by 2.13 g/dl
# For every unit increase in MCV, [Hb] increases by 0.14 g/dl
```

Quick summary of top model comparison: 
- Model 2, full model + species grouping variable fits best by var! R^2 = 0.60. 

Standardized Estimates: 
MCV = 138
TRBC = 1.43

Unstandardized Estimates: 
MCV = 0.14
TRBC = 2.13
- For every increase in MCV of one fl, we expect [Hb] will increase by 0.14 g/dl. 
- For every increase in TRBC of one unit, we expect [Hb] will increase by 2.13 g/dl. 



# 3) species-specific CeNS models and the CeNS index 

# Set up models...all currently in testing phase 
When datasets are large, priors are unlikely to have large influence unless they are highly informative.
Best description of the effect of priors I've seen (look at the  comparison plot at the end!): https://www.rensvandeschoot.com/tutorials/brms-priors/
TAKE HOME: Informative priors pull the posteriors towards them, while uninformarive priors yield a posterior that is centred around what would be the frequentist (LME4) estimate.
```{r}
# get priors
get_prior(hb ~ 1 + mcv + trbc, data = Pguy) 

# Futz w/ priors to get model to fit better 
# Overwrite those (not so) terrible choices with some slightly informed priors.
# Otherwise known as 'let the data drive the model'. 
# prior.Pguy <- c(set_prior("student_t(3, 18.9, 2.5)", class = "Intercept"), # Set intercept prior
#                 set_prior("student_t(3,100, 2.5", class = "b", coef="mcv"),
#                 set_prior("student_t(3,2, 2.5", class = "b", coef="trbc")) # Class b = all responses
# # 
# prior1 <- c(
# prior(normal(0, 10), class = Intercept),
# prior(normal(0, 10), class = b, coef = gender),
# prior(cauchy(0, 10), class = sigma)
# )
# 
# plot(hypothesis(cens.Pguy, "mcv > 0", alpha=0.05)) # see efffect of priors on posterior
# get_variables(cens.Pguy)
# summary(cens.Pguy)
```


# FIT MODELS 
REMEMBER: We aren't doing model comparison, so we won't use standardized inputs. We WANT raw variable values to be able to calculate real-world estimates. 
```{r}
job::job({ # Send this to a job so you can use the console while it runs

  cens.Amel <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Amel,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Amel, file="cens.Amel.RData") # save model
# load("cens.Amel.RData")

  cens.Acas <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Acas,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Acas, file="cens.Acas.RData") # save model
# load("cens.Acas.RData")
  
  cens.Akin <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Akin,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Akin, file="cens.Akin.RData") # save model
# load("cens.Akin.RData")
  
 cens.Aama <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Aama,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Aama, file="cens.Aama.RData") # save model
# load("cens.Aama.RData")

 cens.Clar <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Clar,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Clar, file="cens.Clar.RData") # save model
# load("cens.Clar.RData")

 cens.Coen <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Coen,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Coen, file="cens.Coen.RData") # save model
# load("cens.Coen.RData")

 cens.Ccoe <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Ccoe,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Ccoe, file="cens.Ccoe.RData") # save model
# load("cens.Ccoe.RData")

 cens.Ciri <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Ciri,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Ciri, file="cens.Ciri.RData") # save model
# load("cens.Ciri.RData")

 cens.Cvio <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Cvio,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Cvio, file="cens.Cvio.RData") # save model
# load("cens.Cvio.RData")

 cens.Ccor <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Ccor,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Ccor, file="cens.Ccor.RData") # save model
# load("cens.Ccor.RData")

 cens.Eluc <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Eluc,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Eluc, file="cens.Eluc.RData") # save model
# load("cens.Eluc.RData")

 cens.Econ <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Econ,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Econ, file="cens.Econ.RData") # save model
# load("cens.Econ.RData")

 cens.Fmel <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Fmel,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Fmel, file="cens.Fmel.RData") # save model
# load("cens.Fmel.RData")

 cens.Ghir <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Ghir,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Ghir, file="cens.Ghir.RData") # save model
# load("cens.Ghir.RData")

 cens.Hame <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Hame,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Hame, file="cens.Hame.RData") # save model
# load("cens.Hame.RData")

 cens.Hmic <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Hmic,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Hmic, file="cens.Hmic.RData") # save model
# load("cens.Hmic.RData")

 cens.Hlea <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Hlea,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Hlea, file="cens.Hlea.RData") # save model
# load("cens.Hlea.RData")

 cens.Llaf <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Llaf,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Llaf, file="cens.Llaf.RData") # save model
# load("cens.Llaf.RData")

 cens.Lnun <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Lnun,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Lnun, file="cens.Lnun.RData") # save model
# load("cens.Lnun.RData")

 cens.Meup <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Meup,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Meup, file="cens.Meup.RData") # save model
# load("cens.Meup.RData")

 cens.Mpho <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Mpho,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Mpho, file="cens.Mpho.RData") # save model
# load("cens.Mpho.RData")

 cens.Mtyr <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Mtyr,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Mtyr, file="cens.Mtyr.RData") # save model
# load("cens.Mtyr.RData")

 cens.Ound <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Ound,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Ound, file="cens.Ound.RData") # save model
# load("cens.Ound.RData")

 cens.Oest <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Oest,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Oest, file="cens.Oest.RData") # save model
# load("cens.Oest.RData")

 cens.Omel <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Omel,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Omel, file="cens.Omel.RData") # save model
# load("cens.Omel.RData")

# Patagona gigas N 
cens.Pgign <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Pgign,
  family = gaussian(), # student might fit a bit better than gaussian?
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Pgign, file="cens.Pgign.RData") # save model
# load("cens.Pgign.RData")

# Patagona gigas S 
cens.Pgigs <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Pgigs,
  family = gaussian(), # student might fit a bit better than gaussian?
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Pgigs, file="cens.gigs.RData") # save model
# load("cens.Pgigs.RData")

# Phaethornis guy
  cens.Pguy <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Pguy,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Pguy, file="cens.Pguy.RData") # save model
# load("cens.Pguy.RData")

# Phaethornis malaris
  cens.Pmal <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Pmal,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Pmal, file="cens.Pmal.RData") # save model
# load("cens.Pmal.RData")

# Thalurania furcata
  cens.Tfur <- brm(
  formula = bf(hb ~ 1 + mcv + trbc),
  data = Tfur,
  family = student(),
  cores = 4,
  chains = 4,
  thin = 10,
  warmup = 10000, # default is iter/2; shouldn't ever be larger than iter
  iter = 20000,
  control = list(adapt_delta = 0.99, max_treedepth = 15), # adapt_delta can be between 0.8-1.0; higher helps convergence
  sample_prior = TRUE # save priors
)
save(cens.Tfur, file="cens.Tfur.RData") # save model
# load("cens.Tfur.RData")

})
```


# FIT AND CONVERGENCE 
```{R}
# Get effective sample sizes 
# This is esp important since we're dealing with small sample sizes. 
# Effective Sample (ESS) should be as large as possible; BUT, for most applications, ESS > 1,000 is sufficient for stable 
# estimates (Bürkner, 2017). ESS corresponds to the # indep samples with the same estimation power as the N autocorrelated
# samples. It's a measure of “how much indep info there is in autocorrelated chains” (Kruschke 2015, p182-3).
# bayestestR::effective_sample(cens.Pguy) 
# bayestestR::effective_sample(cens.Mtyr) 
# bayestestR::effective_sample(cens.Pgigs) 
# bayestestR::effective_sample(cens.Pgign) 
# JUST LOOK AT BULK AND TAIL ESS FROM MODEL OUTPUT! 


#### JESSIE LEFT OFF HERE: 
# Add in fit metrics for each species-specific dataset
# then, for posterior predictive checks, combine all species into one massive plot output, maybe 5 x 6 


# Fit 
mcmc_plot(cens.Pguy, type = "trace") 
plot(cens.Pguy, N = 2, ask = FALSE)
mcmc_plot(cens.Pguy, type = "acf_bar") # looks at autocorrelation

mcmc_plot(cens.Mtyr, type = "trace") 
plot(cens.Mtyr, N = 2, ask = FALSE)
mcmc_plot(cens.Mtyr, type = "acf_bar") # looks at autocorrelation

mcmc_plot(cens.Pgigs, type = "trace") 
plot(cens.Pgigs, N = 2, ask = FALSE)
mcmc_plot(cens.Pgigs, type = "acf_bar") # looks at autocorrelation

mcmc_plot(cens.Pgign, type = "trace") 
plot(cens.Pgign, N = 2, ask = FALSE)
mcmc_plot(cens.Pgign, type = "acf_bar") # looks at autocorrelation


# Posterior predictive checks 
bayesplot::pp_check(cens.Pguy, resp = "hb", nsamples = 200) # pull draws from model and cast back to original data 
bayesplot::pp_check(cens.Pguy, nsamples = 100, type = "stat_2d") + theme_bw(base_size = 20) # or stat = "median"

bayesplot::pp_check(cens.Mtyr, resp = "hb", nsamples = 200) 
bayesplot::pp_check(cens.Mtyr, nsamples = 100, type = "stat_2d") + theme_bw(base_size = 20)

bayesplot::pp_check(cens.Pgigs, resp = "hb", nsamples = 200) 
bayesplot::pp_check(cens.Pgigs, nsamples = 100, type = "stat_2d") + theme_bw(base_size = 20)

bayesplot::pp_check(cens.Pgign, resp = "hb", nsamples = 200) 
bayesplot::pp_check(cens.Pgign, nsamples = 100, type = "stat_2d") + theme_bw(base_size = 20)


# Estimates 
#plot(conditional_effects(cens.Pguy), points = TRUE) # conditional effects 
mcmc_plot(cens.Pguy)  
```


Residual plots 
```{r}
# Residual plots 
# typical residual plot but w/ addition of uncertainty associated w/ each residual given by generating draws from the dist 
# associated with each residual
Pguy %>%
  add_residual_draws(cens.Pguy) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

Pguy %>%
  add_predicted_draws(cens.Pguy) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()
# If predicted distribution is uniform, probabilities should be well calibrated

Mtyr %>%
  add_residual_draws(cens.Mtyr) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

Mtyr %>%
  add_predicted_draws(cens.Mtyr) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()

# pgig s
Pgigs %>%
  add_residual_draws(cens.Pgigs) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

Pgigs %>%
  add_predicted_draws(cens.Pgigs) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()

# pgig n
Pgign %>%
  add_residual_draws(cens.Pgign) %>%
  ggplot(aes(x = .row, y = .residual)) +
  stat_pointinterval()

Pgign %>%
  add_predicted_draws(cens.Pgign) %>%
  summarise(
    p_residual = mean(.prediction < hb), # probability residual; Bayesian predictive p-value 
    z_residual = qnorm(p_residual) # quantile residuals 
  ) %>%
  ggplot(aes(sample = z_residual)) +
  geom_qq() +
  geom_abline()
```


Conditional effects plots 
```{r}
color_scheme_set("blue") # nice default
cens.Pguy.p1 <- mcmc_plot(cens.Pguy, pars=c("b_mcv", "b_trbc"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.Pguy: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.Pguy.p1
ggsave(cens.Pguy.p1, filename = "cens.Pguy.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

# Plot conditional effects 
cens.Mtyr.p1 <- mcmc_plot(cens.Mtyr, pars=c("b_mcv", "b_trbc"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.Mtyr: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.Mtyr.p1
ggsave(cens.Mtyr.p1, filename = "cens.Mtyr.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

# Pgig s
cens.Pgigs.p1 <- mcmc_plot(cens.Pgigs, pars=c("b_mcv", "b_trbc"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.Pgigs: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.Pgigs.p1
ggsave(cens.Pgigs.p1, filename = "cens.Pgigs.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

# Pgign
cens.Pgign.p1 <- mcmc_plot(cens.Pgign, pars=c("b_mcv", "b_trbc"),
                                    prob_outer=0.95, # 95% outer CI 
                                    prob=0.50, # 50% inner CI 
                                    point_est="mean") + # mean point est; default is median 
labs(x="Effect on [Hb]", title = "cens.Pgign: Posterior mean estimates & 95% Credible Intervals") + 
scale_y_discrete(labels=c("MCV","TRBC"))  
cens.Pgign.p1
ggsave(cens.Pgign.p1, filename = "cens.Pgign.p1_PosteriorEstimates&CredibleIntervals.pdf", bg="transparent", height=7, width=9, units="in")

```


CeNs Index 
```{r}
# Pull model summaries 
summary(cens.Pguy)
summary(cens.Mtyr)
summary(cens.Pgigs)
summary(cens.Pgign)

# R^2 values 
bayes_R2(cens.Mtyr) # 0.276
bayes_R2(cens.Pguy) # 0.276
bayes_R2(cens.Pgigs) # 0.294
bayes_R2(cens.Pgign) # 0.674

# Cens index: TRBC [3] / MCV [2] + TRBC [3]
cens.index.Pguy <- fixef(cens.Pguy)[3]/(fixef(cens.Pguy)[2]+fixef(cens.Pguy)[3]); cens.index.Pguy
cens.index.Mtyr <- fixef(cens.Mtyr)[3]/(fixef(cens.Mtyr)[2]+fixef(cens.Mtyr)[3]); cens.index.Mtyr
cens.index.Pgigs <- fixef(cens.Pgigs)[3]/(fixef(cens.Pgigs)[2]+fixef(cens.Pgigs)[3]); cens.index.Pgigs
cens.index.Pgign <- fixef(cens.Pgign)[3]/(fixef(cens.Pgign)[2]+fixef(cens.Pgign)[3]); cens.index.Pgign
```




----

# A test of converting between standardized and unstandardized coefficients 
**Standardized:** Necessary when predictors are on many scales; allow for comparison of relative strength of predictors; units are standard deviations (so not really real-world interpretable)
**Unstandardized:** Acceptable when all predictors have the same units; denotes the change in the dependent variable with a unit increment in the independent variable (while holding other predictors constant); units are x and y units. 
= So, each has a different purpose. We need to run our models with standardized inputs, but we want to know unstandardized beta estimates to estimate the strength of each x on y. 
This is best achieved by running the top model with unstandardized predictors and ONLY interpreting the coefficients in terms of unit change for x and y. We can attempt to convert standardized estimates to unstandardized and vice-versa (see trials below), but this results in fairly large margins of error between estimates that are hard to explain. 
```{r}
# # EQUATION FOR CONVERTING STANDARDIZED COEFFICIENTS TO UNSTANDARDIZED: 
# # standardized x1 predictor estimate * (standard deviation of y/standard deviation of x1)
# 
# # EQUATION FOR COVERTING UNSTANDARDIZED TO STANDARDIZED:
# # # b = beta * (SDx/SDy)
# # # where b = standardized coefficient; beta = unstandardized coefficient; SDx = std dev of x predictor; SDy = std dev y resp
# 
# # TEST STANDARDIZED MODEL
# job::job({ # Send this to a job so you can use the console while it runs
# st <- brm(
#   formula = bf(hb ~ 1 + mcv.z + trbc.z + (1|species)),
#   data = censdat,
#   family = gaussian(),
#   cores = 4,
#   chains = 4,
#   thin = 10,
#   warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
#   iter = 10000,
#   control = list(adapt_delta = 0.99, max_treedepth = 15),
#   sample_prior = TRUE # save priors
# )
# 
# # TEST UNSTANDARDIZED MODEL
# un <- brm(
#   formula = bf(hb ~ 1 + mcv + trbc + (1|species)),
#   data = censdat,
#   family = gaussian(),
#   cores = 4,
#   chains = 4,
#   thin = 10,
#   warmup = 5000, # default is iter/2; shouldn't ever be larger than iter
#   iter = 10000,
#   control = list(adapt_delta = 0.99, max_treedepth = 15),
#   sample_prior = TRUE # save priors
# )
# })
# 
# # print summaries
# summary(st)
# summary(un)
# 
# ######
# 
# # Tutorial on converting standardized to unstandardized coefficients from Stack Exchange
# # https://stats.stackexchange.com/questions/235057/convert-standardized-coefficients-to-unstandardized-metric-coefficients-for-li
# set.seed(1)
# d=data.frame(y=1:100,x1=runif(100)+10,x2=rnorm(100)+10)
# d$y=1+ 2 * d$x1 + 3*d$x2 ## easy coef 1 2 3
# d0=d # make a copy
# m=lm(y~.,d0) # fit model
# coef(m) # coefficients are 1 2 3
# d=as.data.frame(scale(d)) # now scale it
# m2=lm(y~.,d) # re-run model
# coef(m2) #oefficients are now: -1.575657e-17  1.834809e-01  9.668451e-01
# 
# # Convert standardized to unstandardized
# coef(m2)['x1']*(sd(d0$y)/sd(d0$x1)) # Convert standardized x1 to unstandardized x1
# coef(m2)['x2']*(sd(d0$y)/sd(d0$x2)) # Convert standardized x2 to unstandardized x2
# coef(m)[1]*sd(d0$y)+mean(d0$y)- # Now determine unstandardized intercept
#   (coef(m)['x1']*sd(d0$y)*mean(d0[['x1']])/sd(d0[['x1']]) +
#    coef(m)['x2']*sd(d0$y)*mean(d0[['x2']])/sd(d0[['x2']]))
# 
# # check with our data
# summary(un)
# summary(st)
# fixef(un)[2] # MCV - unstandardized model estimate
# fixef(st)[2]*(sd(censdat$hb)/sd(censdat$mcv)) # this should give me UNSTANDARDIZED mcv
# 
# fixef(un)[3] # trbc - unstandardized model estimate
# fixef(st)[3]*(sd(censdat$hb)/sd(censdat$trbc)) # this should give me UNSTANDARDIZED mcv
# fixef(un)[1]*sd(censdat$hb)+mean(censdat$hb)- # Now determine unstandardized intercept
#   (fixef(un)[2]*sd(censdat$hb)*mean(censdat[['mcv']])/sd(censdat[['mcv']]) +
#    fixef(un)[3]*sd(censdat$hb)*mean(censdat[['trbc']])/sd(censdat[['trbc']])
#    )
# 
# # Hmm...these estimates seem a little off. Let's explore this a bit more.
# 
# # Convert unstandardized to standardized
# mcv.z.model <- round(fixef(st)[2], 2); mcv.z.model # standardized estimate from model
# mcv.z.hand <- round(fixef(un)["mcv", "Estimate"] * (sd(censdat$mcv) / sd(censdat$hb)), 2); mcv.z.hand # hand estimate
# mcv.z.diff <- mcv.z.model-mcv.z.hand; mcv.z.diff # WHY DO ESTIMATES DIFFER????
# 
# trbc.z.model <- round(fixef(st)[3], 2); trbc.z.model # standardized estimate from model
# trbc.z.hand <- round(fixef(un)["trbc", "Estimate"] * (sd(censdat$trbc) / sd(censdat$hb)), 2); trbc.z.hand # hand estimate
# trbc.z.diff <- trbc.z.model-trbc.z.hand; trbc.z.diff # WHY DO ESTIMATES DIFFER????
# 
# # Convert standardized to unstandardized - for this convertion divide std(y)/std(x)
# mcv.unest.model <- round(fixef(un)[2], 2); mcv.unest.model # MCV - unstandardized model estimate
# mcv.unest.hand <- round(fixef(st)[2]*(sd(censdat$hb)/sd(censdat$mcv)), 2); mcv.unest.hand # hand est
# mcv.unest.diff <- mcv.unest.model-mcv.unest.hand; mcv.unest.diff
# 
# trbc.unest.model <- round(fixef(un)[3], 2); trbc.unest.model # trbc - unstandardized model estimate
# trbc.unest.hand <- round(fixef(st)[3]*(sd(censdat$hb)/sd(censdat$trbc)), 2); trbc.unest.hand # hand est
# trbc.unest.diff <- trbc.unest.model-trbc.unest.hand; trbc.unest.diff
# 
# fixef(hb.m4)[-1, ] %>% round(3) # this says: print coefficients but minus the intercept
# 
# ####
# 
# # This post also explains how to convert estimates:
# # https://github.com/ASKurz/Doing-Bayesian-Data-Analysis-in-brms-and-the-tidyverse/blob/master/17.Rmd
# # Based on Kruschke's Equation 17.2, convert standardized coefficients back to their original metric using the formulas
# 
# # \begin{align*}
# # \beta_0 & = \zeta_0 \operatorname{SD}_y + M_y - \frac{\zeta_1 M_x \operatorname{SD}_y}{\operatorname{SD}_x} \;\; \text{and}  \\
# # \beta_1 & = \frac{\zeta_1 \operatorname{SD}_y}{\operatorname{SD}_x},
# # \end{align*}
# #
# # where $\zeta_0$ and $\zeta_1$ denote the intercept and slope for the model of the standardized data, respectively, and that model follows the familiar form
# #
# # $$z_{\hat y} = \zeta_0 + \zeta_1 z_x.$$
# # To implement those equations, we'll first extract the posterior samples. We begin with `fit17.1`, the model for which $N = 300$.
# 
# post <- posterior_samples(st)
# head(post)
# 
# # Let's wrap the consequences of Equation 17.2 into two functions.
# make_beta_0 <- function(zeta_0, zeta_1, sd_x, sd_y, m_x, m_y) {
#   zeta_0 * sd_y + m_y - zeta_1 * m_x * sd_y / sd_x # zeta_0 = intercept for standardized data 
# }
# make_beta_1 <- function(zeta_1, sd_x, sd_y) {
#   zeta_1 * sd_y / sd_x # zeta_1 = slope for standardized data (aka, estimate of predictor)
# }
# # where $\zeta_0$ and $\zeta_1$ denote the intercept and slope for the model of the standardized data
# 
# # After saving a few values, we're ready to use our custom functions to convert our posteriors for `b_Intercept` and `b_height_z` # to their natural metric.
# sd_x <- sd(censdat$mcv)
# sd_y <- sd(censdat$hb)
# m_x  <- mean(censdat$mcv)
# m_y  <- mean(censdat$hb)
# 
# post <- # This converts ALL model point estimates to unstandardized form. 
#   post %>% 
#   mutate(b_0 = make_beta_0(zeta_0 = b_Intercept,
#                            zeta_1 = b_mcv.z,
#                            sd_x   = sd_x,
#                            sd_y   = sd_y,
#                            m_x    = m_x,
#                            m_y    = m_y),
#          b_1 = make_beta_1(zeta_1 = b_mcv.z,
#                            sd_x   = sd_x,
#                            sd_y   = sd_y))
# glimpse(post)
# 
# # This example  confirms that yes, I'm doing the math write...but I still can't figure out why hand-estimated and model-
# # estimated numbers differ. 
```

**Conclusion** after I did quite a bit of digging: Few people convert standardized coefficients to unstandardized, but many convert unstandardized to standardized. 

While it's possible to estimate unstandardized coefficients from standardized, I come up with measurement differences between the hand-estimated and model-estimated unstandardized coefficients, and I don't like that. It seems the best approach is to: 
1) Run model sets with standardized coefficients to determine differences among predictors and strength of effects; generate posterior estimates and plots from these models --> i.e., allows you to say elevation has a stronger effect than temperature on Hb. 
2) Run the top model with unstandardized coefficients. Use this model summary to estimate the magnitude of change in y relative to each x, but DO NOT compare among x variables. --> i.e., does not allow you to compare the magnitude of effect of elevation vs. temperature on Hb, but DOES allow you to say that for every 100 m increase in elev you expect [hb] to increase by 1.0 g/dl, for example (these are not real numbers). 

More on these topics here: 
https://www.researchgate.net/post/Standardized-vs-Unstandardized-regression-coefficients
"...independent variable with a larger standardized coefficient will have a greater effect on the dependent variable.
    in interpretation of your regression results the value of any unstandardized coefficient denotes the change in the dependent variable with a unit increment in the independent variable. But you can not compare them in terms of impact on the dependent variable.
    Unstandardized coefficients are useful in interpretation and standardized coefficients in comparison of impact of any independent variable on the dependent variable.

https://www.researchgate.net/post/What-is-the-difference-between-standardized-and-unstandardized-regression-coefficient
"Unstandardized regression coefficients tell you how much change in Y (the amount is B) is predicted/estimated to occur per unit change in that independent variable (X), when all other IVs are held constant. These retain the individual scales of the IVs and the DV.
    Standardized regression coefficients tell you how much change in Y (the amount is the "beta", representing number of standard deviations) is predicted/estimated per unit (SD) change in that X, when all other IVs are held constant. These values pertain only when all variables are scaled to z-scores (mean = 0, SD = 1), and do not represent the original scale(s).
    Comparison of relative emphasis given to individual IVs is only possible when all are on a common scale, so standardized coefficients would be the ones to look at when you are trying to determine relative weight given to each IV."





# Print environment for reproducibility
```{r}
sessionInfo() # List of packages and versions in use 
```

###########

## END 

